{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7961f32a",
   "metadata": {},
   "source": [
    "### Libraries installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install joblib\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "from langdetect import detect\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "from keras.layers import Embedding, Layer, Dense, Dropout, MultiHeadAttention, LayerNormalization, Input, GlobalAveragePooling1D, LSTM, Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a7e5e",
   "metadata": {},
   "source": [
    "### Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8504bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_a_meses(fecha_str):\n",
    "    # Using regular expressions to find numbers in the string\n",
    "    numeros = re.findall(r'\\d+', fecha_str)\n",
    "    if not numeros:\n",
    "        if \"un\" in fecha_str.lower(): # If the string contains \"one\", convert it to 1\n",
    "            numero = 1\n",
    "        else:\n",
    "            return None  # If no numbers or \"one\" found, return None\n",
    "    else:\n",
    "        numero = int(numeros[0])  # Take the first found number\n",
    "    \n",
    "     # Checking if the string contains 'año' or 'mes'\n",
    "    if 'año' in fecha_str:\n",
    "        return numero * 12\n",
    "    elif 'mes' in fecha_str:\n",
    "        return numero\n",
    "    elif 'semana' in fecha_str:\n",
    "        return numero * 4  # Considering a week as 4 weeks\n",
    "    elif 'día' in fecha_str:\n",
    "        return numero / 30  # Considering a day as 1/30 of a month\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def top_words(text, N):\n",
    "    all_words = ' '.join(text).split()\n",
    "    freq_dist = nltk.FreqDist(all_words)\n",
    "    top_words = [word for word, _ in freq_dist.most_common(N)]\n",
    "    return top_words\n",
    "def es_espanol(texto):\n",
    "    try:\n",
    "        return detect(texto) == 'es'\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d725c8",
   "metadata": {},
   "source": [
    "### Dowload and preprocessing of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5771673",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('TODAS_ESTACIONES.xlsx')\n",
    "df = df[df['Review'].apply(es_espanol)]\n",
    "df = df.drop_duplicates(subset=['Review'])\n",
    "df = df.dropna(subset=['Review', 'Real'])\n",
    "\n",
    "# Remove duplicate rows in 'Review' column. \n",
    "resultados = df.drop_duplicates(subset=['Review'])\n",
    "\n",
    "# Remove NA rows in 'Review' and 'Real' columns. \n",
    "resultados = resultados.dropna(subset=['Review', 'Real'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Download the list of Spanish stop words from NLTK.\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "spanish_stopwords = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c62478c",
   "metadata": {},
   "source": [
    "## Experiments with SVM classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b426a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the StratifiedKFold object for cross-validation\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store results per category\n",
    "precision_by_category = []\n",
    "recall_by_category = []\n",
    "f1_score_by_category = []\n",
    "roc_auc_score_by_category = []\n",
    "confusion_matrices = []\n",
    "X = resultados['Review']\n",
    "y = resultados['Real']\n",
    "X = X.tolist()\n",
    "y = y.tolist()\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in stratified_kfold.split(X, y):\n",
    "    X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
    "    y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "    \n",
    "    # Create a pipeline including feature extraction (TF-IDF) and classifier (SVM)\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words=spanish_stopwords)),  # Configurar el vectorizador TF-IDF con stop words en español\n",
    "        ('clf', SVC(kernel='linear'))  # Usar un kernel lineal para permitir la interpretación de los coeficientes\n",
    "    ])\n",
    "\n",
    "    # Resample the training dataset to balance the classes\n",
    "    oversampler = RandomOverSampler(random_state=321)\n",
    "    X_train_resampled, y_train_resampled = oversampler.fit_resample(np.array(X_train).reshape(-1, 1), y_train)\n",
    "\n",
    "    # Train the classifier with the resampled training dataset\n",
    "    pipeline.fit(X_train_resampled.ravel(), y_train_resampled)\n",
    "\n",
    "    # Evaluate classifier performance\n",
    "    predictions = pipeline.predict(X_test)\n",
    "\n",
    "    # Calculate and store metrics per category\n",
    "    precision_by_category.append(precision_score(y_test, predictions, average=None))\n",
    "    recall_by_category.append(recall_score(y_test, predictions, average=None))\n",
    "    f1_score_by_category.append(f1_score(y_test, predictions, average=None))\n",
    "    roc_auc_score_by_category.append(roc_auc_score(y_test, predictions, average=None))\n",
    "    \n",
    "    # Calculate confusion matrix for this fold\n",
    "    confusion_matrices.append(confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Calculate mean metrics per category\n",
    "mean_precision = np.mean(precision_by_category, axis=0)\n",
    "mean_recall = np.mean(recall_by_category, axis=0)\n",
    "mean_f1_score = np.mean(f1_score_by_category, axis=0)\n",
    "mean_roc_auc_score = np.mean(roc_auc_score_by_category, axis=0)\n",
    "\n",
    "# Print mean metrics per categoryprint(\"Mean Precision:\", mean_precision)\n",
    "print(\"Mean Recall:\", mean_recall)\n",
    "print(\"Mean F1 Score:\", mean_f1_score)\n",
    "print(\"Mean ROC AUC Score:\", mean_roc_auc_score)\n",
    "\n",
    "# Calculate mean confusion matrixmean_confusion_matrix = np.mean(confusion_matrices, axis=0)\n",
    "\n",
    "# Print mean confusion matrix\n",
    "print(\"Mean Confusion Matrix:\\n\", mean_confusion_matrix)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(pipeline, 'modelo_svm_all.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45089b25",
   "metadata": {},
   "source": [
    "## Experiments with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8955ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the StratifiedKFold object for cross-validation\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store results per category\n",
    "precision_by_category = []\n",
    "recall_by_category = []\n",
    "f1_score_by_category = []\n",
    "roc_auc_score_by_category = []\n",
    "confusion_matrices = []\n",
    "X = resultados['Review']\n",
    "y = resultados['Real']\n",
    "X = X.tolist()\n",
    "y = y.tolist()\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in stratified_kfold.split(X, y):\n",
    "    X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
    "    y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "    \n",
    "    # Create a pipeline including feature extraction (TF-IDF) and classifier (Random Forest)\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words=spanish_stopwords)),  # Configurar el vectorizador TF-IDF con stop words en español\n",
    "        ('clf', RandomForestClassifier(n_estimators=1000, random_state=2345))  # Usar un kernel lineal para permitir la interpretación de los coeficientes\n",
    "    ])\n",
    "\n",
    "    # Resample the training dataset to balance the classes\n",
    "    oversampler = RandomOverSampler(random_state=321)\n",
    "    X_train_resampled, y_train_resampled = oversampler.fit_resample(np.array(X_train).reshape(-1, 1), y_train)\n",
    "\n",
    "    # Train the classifier with the resampled training dataset\n",
    "    pipeline.fit(X_train_resampled.ravel(), y_train_resampled)\n",
    "\n",
    "    # Evaluate classifier performance\n",
    "    predictions = pipeline.predict(X_test)\n",
    "\n",
    "   # Calculate and store metrics per category\n",
    "    precision_by_category.append(precision_score(y_test, predictions, average=None))\n",
    "    recall_by_category.append(recall_score(y_test, predictions, average=None))\n",
    "    f1_score_by_category.append(f1_score(y_test, predictions, average=None))\n",
    "    roc_auc_score_by_category.append(roc_auc_score(y_test, predictions, average=None))\n",
    "    \n",
    "    # Calculate confusion matrix for this fold\n",
    "    confusion_matrices.append(confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Calculate mean metrics per category\n",
    "mean_precision = np.mean(precision_by_category, axis=0)\n",
    "mean_recall = np.mean(recall_by_category, axis=0)\n",
    "mean_f1_score = np.mean(f1_score_by_category, axis=0)\n",
    "mean_roc_auc_score = np.mean(roc_auc_score_by_category, axis=0)\n",
    "\n",
    "# Print mean metrics per category\n",
    "print(\"Mean Precision:\", mean_precision)\n",
    "print(\"Mean Recall:\", mean_recall)\n",
    "print(\"Mean F1 Score:\", mean_f1_score)\n",
    "print(\"Mean ROC AUC Score:\", mean_roc_auc_score)\n",
    "\n",
    "# Calculate mean confusion matrix\n",
    "mean_confusion_matrix = np.mean(confusion_matrices, axis=0)\n",
    "\n",
    "# Print mean confusion matrix\n",
    "print(\"Mean Confusion Matrix:\\n\", mean_confusion_matrix)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(pipeline, 'modelo_rf_all.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8547db5",
   "metadata": {},
   "source": [
    "## Experiments with Transformer Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae85f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = resultados['Review']\n",
    "y = resultados['Real']\n",
    "\n",
    "# Split data in training and testnig\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ac8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the parameters for Transformer Network\n",
    "max_len = 20       \n",
    "oov_token = '00_V' \n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'  \n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocab Size: \",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a779016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform text data into numerical data (Embedding)\n",
    "X_train_list = X_train.tolist()\n",
    "X_test_list = X_test.tolist()\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train_list)\n",
    "X_train = pad_sequences(train_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test_list)\n",
    "X_test = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5494ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define classes required\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = ops.shape(x)[-1]\n",
    "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        \n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]  # Utilizar tf.shape en lugar de ops.shape\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)  # Utilizar tf.range en lugar de ops.arange\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d91b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop Transformer model\n",
    "embed_dim = 256  # Embedding size for each token\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 64  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(20,))\n",
    "embedding_layer = TokenAndPositionEmbedding(20, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c61f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Define the StratifiedKFold object for cross-validation\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store results per category\n",
    "precision_by_category = []\n",
    "recall_by_category = []\n",
    "f1_score_by_category = []\n",
    "roc_auc_score_by_category = []\n",
    "confusion_matrices = []\n",
    "X = resultados['Review']\n",
    "y = resultados['Real']\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in stratified_kfold.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]  \n",
    "\n",
    "    \n",
    "    # Convert text data to numerical sequences and apply padding\n",
    "    X_train_list = X_train.tolist()\n",
    "    X_test_list = X_test.tolist()\n",
    "    train_sequences = tokenizer.texts_to_sequences(X_train_list)\n",
    "    X_train_padded = pad_sequences(train_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    test_sequences = tokenizer.texts_to_sequences(X_test_list)\n",
    "    X_test_padded = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    embed_dim = 256  # Embedding size for each token\n",
    "    num_heads = 4  # Number of attention heads\n",
    "    ff_dim = 64  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "    inputs = layers.Input(shape=(20,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(20, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(20, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train the model on the current fold\n",
    "    model.fit(X_train_padded, y_train, batch_size=32, epochs=100, verbose=1)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    predictions = model.predict(X_test_padded)\n",
    "    \n",
    "    # Convert probability predictions to classes (0 or 1)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "   # Calculate and store metrics per category\n",
    "    precision_by_category.append(precision_score(y_test, predicted_classes, average=None))\n",
    "    recall_by_category.append(recall_score(y_test, predicted_classes, average=None))\n",
    "    f1_score_by_category.append(f1_score(y_test, predicted_classes, average=None))\n",
    "    roc_auc_score_by_category.append(roc_auc_score(y_test, predicted_classes, average=None))\n",
    "    \n",
    "    #  Calculate confusion matrix for this fold\n",
    "    confusion_matrices.append(confusion_matrix(y_test, predicted_classes))\n",
    "\n",
    "# Calculate mean metrics per category\n",
    "mean_precision = np.mean(precision_by_category, axis=0)\n",
    "mean_recall = np.mean(recall_by_category, axis=0)\n",
    "mean_f1_score = np.mean(f1_score_by_category, axis=0)\n",
    "mean_roc_auc_score = np.mean(roc_auc_score_by_category, axis=0)\n",
    "\n",
    "# Print mean metrics per category\n",
    "print(\"Mean Precision:\", mean_precision)\n",
    "print(\"Mean Recall:\", mean_recall)\n",
    "print(\"Mean F1 Score:\", mean_f1_score)\n",
    "print(\"Mean ROC AUC Score:\", mean_roc_auc_score)\n",
    "\n",
    "# Calculate mean confusion matrix\n",
    "mean_confusion_matrix = np.mean(confusion_matrices, axis=0)\n",
    "\n",
    "# Print mean confusion matrix\n",
    "print(\"Mean Confusion Matrix:\\n\", mean_confusion_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
